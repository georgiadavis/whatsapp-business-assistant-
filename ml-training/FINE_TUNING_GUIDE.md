# ðŸš€ Fine-Tuning Setup Guide

This guide will walk you through fine-tuning your Llama model for the WhatsApp Business Assistant.

## ðŸ“‹ Prerequisites

âœ… Python 3.9+ installed
âœ… Training data prepared (6,903 examples)
âœ… Llama API key from your provider
âœ… Required packages installed

## ðŸ”§ Step 1: Install Required Packages

Already done! âœ…
- `llama-stack-client`
- `python-dotenv`
- `requests`
- `beautifulsoup4`

## ðŸ” Step 2: Set Up Your API Key (IMPORTANT!)

### DO NOT share your API key in chat! Follow these steps:

1. **Copy the example environment file:**
   ```bash
   cd ml-training
   cp .env.example .env
   ```

2. **Edit the `.env` file with your API key:**
   ```bash
   # Open .env in your text editor
   code .env
   # or
   nano .env
   ```

3. **Replace `your_api_key_here` with your actual Llama API key:**
   ```env
   LLAMA_API_KEY=sk-your-actual-api-key-here
   ```

4. **Save the file**

The `.env` file is protected by `.gitignore` and will NOT be committed to version control. âœ…

## ðŸ“Š Step 3: Verify Your Training Data

Your training data is ready:
- **Location:** `data/training/training_data_complete.jsonl`
- **Total Examples:** 6,903
- **Format:** Chat-style messages for Llama

### Data Breakdown:
- Intent classification: 3,436 examples
- Reasoning generation: 3,436 examples
- Help article Q&A: 31 examples

## ðŸŽ¯ Step 4: Configure Fine-Tuning (Optional)

Edit `.env` to customize training parameters:

```env
# Model Configuration
MODEL_NAME=llama-3.1-8b
OUTPUT_MODEL_NAME=whatsapp-business-assistant-v1

# Training Hyperparameters
LEARNING_RATE=2e-5      # Lower = more stable, Higher = faster
BATCH_SIZE=4            # Higher = faster but needs more memory
NUM_EPOCHS=3            # More epochs = more training
MAX_SEQ_LENGTH=2048     # Maximum tokens per example
```

**Recommended settings (already set):**
- Learning rate: `2e-5` (good balance)
- Batch size: `4` (works on most GPUs)
- Epochs: `3` (prevents overfitting)

## ðŸš€ Step 5: Start Fine-Tuning

Once your `.env` file is configured with your API key:

```bash
cd ml-training/scripts
/usr/bin/python3 fine_tune_llama.py
```

### What happens:
1. âœ… Validates your training data format
2. âœ… Uploads data to Llama Stack
3. âœ… Starts the fine-tuning job
4. âœ… Saves job details to `outputs/fine_tuning_job.json`

## ðŸ“Š Step 6: Monitor Training Progress

### Check status anytime:

```bash
cd ml-training/scripts
/usr/bin/python3 check_fine_tuning_status.py
```

Or with a specific job ID:

```bash
/usr/bin/python3 check_fine_tuning_status.py job-abc123
```

### Status meanings:
- **pending** â³ - Job is queued
- **running** â³ - Training in progress
- **succeeded** ðŸŽ‰ - Training complete!
- **failed** âŒ - Something went wrong

## â° How Long Does It Take?

Fine-tuning time depends on:
- **Dataset size:** 6,903 examples
- **Model size:** Llama 3.1 8B
- **Epochs:** 3

**Estimated time:** 2-6 hours (varies by provider)

You'll receive an email notification when training completes.

## ðŸŽ‰ Step 7: Use Your Fine-Tuned Model

Once training succeeds, you'll get a model ID like:
```
whatsapp-business-assistant-v1:ft-abc123
```

Use this model ID for inference in your Android app!

## ðŸ” Troubleshooting

### Error: "API key not set"
- Make sure you created `.env` from `.env.example`
- Verify your API key is correct in `.env`
- Don't include quotes around the API key

### Error: "Training file not found"
- Run from `ml-training/scripts/` directory
- Check that `data/training/training_data_complete.jsonl` exists

### Error: "Invalid JSON format"
- Your training data might be corrupted
- Re-run `merge_training_data.py` to regenerate

### Training failed
- Check error message in status output
- Verify training data format
- Check API key permissions
- Contact Llama support if issue persists

## ðŸ“ File Structure

```
ml-training/
â”œâ”€â”€ .env                     âš ï¸  YOUR API KEY (keep secret!)
â”œâ”€â”€ .env.example             ðŸ“‹ Template
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fine_tune_llama.py   ðŸš€ Main fine-tuning script
â”‚   â””â”€â”€ check_fine_tuning_status.py  ðŸ“Š Status checker
â”œâ”€â”€ data/
â”‚   â””â”€â”€ training/
â”‚       â””â”€â”€ training_data_complete.jsonl  ðŸ“š Your data
â””â”€â”€ outputs/
    â””â”€â”€ fine_tuning_job.json  ðŸ’¾ Job info
```

## ðŸ›¡ï¸ Security Best Practices

âœ… Never commit `.env` to version control
âœ… Never share API keys in chat or code
âœ… Store API keys in environment variables
âœ… Use `.env.example` for templates only
âœ… Rotate API keys periodically

## ðŸ“š Next Steps After Fine-Tuning

1. âœ… Test your fine-tuned model
2. âœ… Integrate into Android app
3. âœ… Deploy to production
4. âœ… Monitor performance
5. âœ… Collect feedback and iterate

## ðŸ’¡ Tips for Better Results

- **More data = better results:** Consider adding more help articles
- **Quality > Quantity:** Focus on accurate, helpful responses
- **Balanced dataset:** Ensure all intent categories are represented
- **Test thoroughly:** Validate model responses before production
- **Iterate:** Fine-tune multiple times with different data mixes

## ðŸ†˜ Need Help?

- Check Llama Stack documentation
- Review error messages carefully
- Test with smaller dataset first
- Join Llama community forums

---

**Ready to start?** Make sure your `.env` file is configured and run:

```bash
cd ml-training/scripts
/usr/bin/python3 fine_tune_llama.py
```

Good luck! ðŸš€
